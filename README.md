
# ğŸ’¼ Mini E-Ticaret + Kafka + Airflow Projesi

Bu proje, kÃ¼Ã§Ã¼k bir e-ticaret web uygulamasÄ± Ã¼zerinden yapÄ±lan satÄ±ÅŸlarÄ± Kafka ile mesaj kuyruÄŸuna gÃ¶nderir ve Airflow ile her gÃ¼n saat **15:30**'da bu satÄ±ÅŸlarÄ± CSV formatÄ±nda iÅŸler.

## ğŸ”§ Proje BileÅŸenleri

### 1. `app/` â€“ Flask Web UygulamasÄ±

- Basit bir Ã¼rÃ¼n listesi gÃ¶rÃ¼ntÃ¼lenir.
- "SatÄ±n Al" butonuna tÄ±klandÄ±ÄŸÄ±nda Kafka'ya satÄ±ÅŸ mesajÄ± gÃ¶nderilir.
- Flask uygulamasÄ± port **5000**'de Ã§alÄ±ÅŸÄ±r.

### 2. `airflow/` â€“ Apache Airflow

- `dags/process_sales_dag.py`: Kafkaâ€™dan verileri okuyup `/opt/airflow/data/sales` dizinine CSV olarak yazan DAG.
- Bu DAG her gÃ¼n **15:30 yerel saat**'te tetiklenir.

### 3. `kafka/` â€“ Apache Kafka & Zookeeper

- `sales_topic` adÄ±nda bir topic vardÄ±r.
- Flask uygulamasÄ± satÄ±ÅŸlarÄ± buraya Ã¼retir.
- Airflow, bu topic'ten satÄ±ÅŸ verilerini tÃ¼ketir.

## ğŸ“¦ Docker Servisleri (`docker-compose.yml`)

| Servis              | AÃ§Ä±klama                                                         |
|---------------------|------------------------------------------------------------------|
| `web`               | Flask uygulamasÄ±nÄ± barÄ±ndÄ±rÄ±r.                                   |
| `zookeeper`         | Kafka iÃ§in gerekli servis.                                       |
| `kafka`             | Kafka broker.                                                    |
| `kafka-ui`          | Kafka topiclerini ve mesajlarÄ± gÃ¶rÃ¼ntÃ¼lemek iÃ§in web arayÃ¼zÃ¼.    |
| `postgres`          | Airflow iÃ§in metadata veritabanÄ±.                                |
| `redis`             | Celery iÃ§in mesaj kuyruÄŸu.                                       |
| `airflow-init`      | Airflow veritabanÄ±nÄ± baÅŸlatÄ±r ve admin kullanÄ±cÄ± oluÅŸturur.      |
| `airflow-webserver` | Airflow arayÃ¼zÃ¼.                                                 |
| `airflow-scheduler` | DAGâ€™leri tetikleyen servis.                                      |
| `airflow-worker`    | Airflow gÃ¶revlerini Ã§alÄ±ÅŸtÄ±rÄ±r.                                  |

## ğŸ“ Proje KlasÃ¶r YapÄ±sÄ±

```
E-TÄ°CARET/

â”œâ”€â”€ airflow/                  # Airflow iÃ§in Dockerfile veya config dosyalarÄ±
â”‚   â””â”€â”€ Dockerfile            # Airflow image'Ä±nÄ± Ã¶zelleÅŸtirmek iÃ§in kullanÄ±lÄ±r
â”‚
â”œâ”€â”€ app/                      # Flask web uygulamasÄ±
â”‚   â”œâ”€â”€ templates/            # HTML ÅŸablonlarÄ±
â”‚   â”‚   â””â”€â”€ index.html        # ÃœrÃ¼n listeleme ve satÄ±n alma sayfasÄ±
â”‚   â”œâ”€â”€ app.py                # Flask uygulamasÄ±nÄ±n giriÅŸ noktasÄ±
â”‚   â””â”€â”€ Dockerfile            # Flask uygulamasÄ±nÄ± container iÃ§inde Ã§alÄ±ÅŸtÄ±rmak iÃ§in Dockerfile
â”‚
â”œâ”€â”€ dags/                     # Airflow DAG dosyalarÄ±
â”‚   â””â”€â”€ process_sales_dag.py  # GÃ¼nlÃ¼k satÄ±ÅŸ verilerini Kafka'dan okuyup CSV'ye yazan DAG
â”‚
â”œâ”€â”€ data/                     # UygulamanÄ±n iÅŸlediÄŸi verilerin bulunduÄŸu klasÃ¶r
â”‚   â””â”€â”€ sales/                # GÃ¼nlÃ¼k satÄ±ÅŸ verilerinin CSV dosyalarÄ± buraya yazÄ±lÄ±r
â”‚
â”œâ”€â”€ kafka/                    # Kafka producer/consumer scriptleri
â”‚   â”œâ”€â”€ producer.py           # ÃœrÃ¼n satÄ±n alÄ±ndÄ±ÄŸÄ±nda Kafkaâ€™ya mesaj yollayan script
â”‚   â””â”€â”€ consumer.py           # Kafkaâ€™dan veri okumak iÃ§in kullanÄ±labilir
â”‚
â”œâ”€â”€ logs/                     # Airflow log dosyalarÄ±
â”‚
â”œâ”€â”€ plugins/                  # (Opsiyonel) Airflow Ã¶zel pluginâ€™leri iÃ§in klasÃ¶r
â”‚
â”œâ”€â”€ docker-compose.yaml       # TÃ¼m servisleri tanÄ±mlayan Docker Compose konfigÃ¼rasyonu

```

## ğŸ•’ Zamanlama

- Airflow DAGâ€™i yerel saatle **15:30**'da Ã§alÄ±ÅŸacak ÅŸekilde ayarlanmÄ±ÅŸtÄ±r.
- UTC+3

## ğŸ› ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1. Proje dizinine gidin:
   ```bash
   cd -projedizini-
   ```

2. Gerekli Docker imajlarÄ±nÄ± oluÅŸturun:
   ```bash
   docker-compose build
   ```

3. Servisleri baÅŸlatÄ±n:
   ```bash
   docker-compose up -d
   ```

4. Uygulamalara eriÅŸin:

   - Flask: [http://localhost:5000](http://localhost:5000)  
   - Kafka UI: [http://localhost:8080](http://localhost:8080)  
   - Airflow UI: [http://localhost:8081](http://localhost:8081)
  
5. ğŸ” Airflow FERNET_KEY Nedir?
Airflow, baÄŸlantÄ± bilgileri gibi hassas verileri veri tabanÄ±nda ÅŸifreleyerek saklar. Bu iÅŸlemi yapabilmek iÃ§in FERNET ÅŸifreleme anahtarÄ±na ihtiyaÃ§ duyar.

Yani, FERNET_KEY, Airflow'un gÃ¼venlik iÃ§in kullandÄ±ÄŸÄ± ÅŸifreleme/deÅŸifreleme anahtarÄ±dÄ±r.

ğŸ”‘ NasÄ±l OluÅŸturulur?
Yeni bir FERNET_KEY Ã¼retmek iÃ§in aÅŸaÄŸÄ±daki Python komutunu terminalde Ã§alÄ±ÅŸtÄ±rabilirsiniz:

```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```
Elde edilen bu anahtar, docker-compose.yml dosyasÄ±ndaki tÃ¼m Airflow servislerinin environment kÄ±smÄ±na ÅŸu ÅŸekilde yazÄ±lmalÄ±dÄ±r:

```bash
AIRFLOW__CORE__FERNET_KEY: '***'
```
â— Not: FERNET anahtarÄ± tÃ¼m servislerde aynÄ± olmalÄ±dÄ±r (webserver, scheduler, worker), aksi halde Airflow ÅŸifrelenmiÅŸ verileri okuyamaz ve hata alÄ±rsÄ±nÄ±z.

## ğŸ“„ CSV DosyasÄ±na EriÅŸim

CSV dosyasÄ± Airflow worker konteynerinde `/opt/airflow/data/sales` klasÃ¶rÃ¼ne kaydedilir. DosyayÄ± host makinenize almak iÃ§in:

```bash
docker cp airflow_worker:/opt/airflow/data/sales/daily_sales_YYYY-MM-DD.csv ./data/sales/
```

> **Not:** EÄŸer `./data/sales` klasÃ¶rÃ¼nÃ¼zde CSV gÃ¶rÃ¼nmÃ¼yorsa, `airflow-worker` servisine ÅŸu volume'Ã¼ eklediÄŸinizden emin olun:

```yaml
volumes:
  - ./data/sales:/opt/airflow/data/sales
```

Bu deÄŸiÅŸiklik sonrasÄ± sistemi ÅŸu ÅŸekilde yeniden baÅŸlatabilirsiniz:

```bash
docker-compose down -v
docker-compose up --build -d
```
## ğŸ“¸ Proje GÃ¶rselleri


![web](https://github.com/user-attachments/assets/b79f853d-e5c4-4faa-92c7-c85f064ad3b2)
![realairflow](https://github.com/user-attachments/assets/e957c14e-5cb2-4345-ac95-e86e3ba068e7)
![kafka](https://github.com/user-attachments/assets/4f4d6a89-2636-4429-8a4e-a8496b67e17a)

